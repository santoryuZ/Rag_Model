{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7941afb0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6917f20a",
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GOOGLE_API_KEY\"\n",
        "# To activate LangSmith.\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" \n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR_LANGCHAIN_API_KEY\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"My_Rag_Model\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a648fc3d",
      "metadata": {},
      "source": [
        "### Initializing a LLM Model for our Model to access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06586a06",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"CALLING THE LLM INSIDE THE LANGCHAIN\"\"\"\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI # Importing the chat model.\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\") # Selecting the model we want to use. \n",
        "\n",
        "llm_response = llm.invoke(\"Tell me a good joke\") # Asking something to th llm model (invoking).\n",
        "\n",
        "print(llm_response.content) # Getting the response."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acb4acf8",
      "metadata": {},
      "source": [
        "### An Output Parser to Parse our Output so that we could get only the relevant Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f8537c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"PARSING THE OUTPUT\"\"\"\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "output_parser.invoke(llm_response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d4aaf64",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"CREATING A SIMPLE CHAIN\"\"\"\n",
        "chain = llm | output_parser\n",
        "chain.invoke(\"Tell me a Joke.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0f6d9e3",
      "metadata": {},
      "source": [
        "### Structured Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96f940bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class MobileReview(BaseModel):\n",
        "    phone_model: str = Field(description=\"Name and model of the phone\")\n",
        "    rating: float = Field(description=\"Overall rating out of 5\")\n",
        "    pros: List[str] = Field(description=\"List of positive aspects\")\n",
        "    cons: List[str] = Field(description=\"List of negative aspects\")\n",
        "    summary: str = Field(description=\"Brief summary of the review\")\n",
        "\n",
        "review_text = \"\"\"\n",
        "Just got my hands on the new Galaxy S21 and wow, this thing is slick! The screen is gorgeous,\n",
        "colors pop like crazy. Camera's insane too, especially at night - my Insta game's never been\n",
        "stronger. Battery life's solid, lasts me all day no problem.\n",
        "Not gonna lie though, it's pretty pricey. And what's with ditching the charger? C'mon Samsung.\n",
        "Also, still getting used to the new button layout, keep hitting Bixby by mistake.\n",
        "Overall, I'd say it's a solid 4 out of 5. Great phone, but a few annoying quirks keep it from\n",
        "being perfect. If you're due for an upgrade, definitely worth checking out!\n",
        "\"\"\"\n",
        "\n",
        "structured_llm = llm.with_structured_output(MobileReview)\n",
        "output = structured_llm.invoke(review_text)\n",
        "print(output)\n",
        "print(output.pros)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5538e21e",
      "metadata": {},
      "source": [
        "### Prompt Template \n",
        "###### How langchain allows us to specify placeholders for some input text. So the the templates are nothing but those strings with the placeholders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d84c385",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
        "prompt.invoke({\"topic\": \"programming\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c00c286",
      "metadata": {},
      "outputs": [],
      "source": [
        "chain = prompt | llm | output_parser\n",
        "chain.invoke({\"topic\": \"programmer\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6aeff4b",
      "metadata": {},
      "source": [
        "### LLM Messages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47d5d5f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ways of creating a Prompt Template.\n",
        "# Using Human and System message.\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "system_message = SystemMessage(content= \"You are a helpful assistant that tells jokes.\") # Defines the character or behaviour of the bot or agent that we are creating.\n",
        "# Therefore we see some character defining content in there\n",
        "\n",
        "human_message = HumanMessage(content= \"Tell me a joke about programming\")\n",
        "response = llm.invoke([system_message, human_message])\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "414804ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Similar way but without directly using System and Human functions.\n",
        "template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful assistant that tells jokes.\"),\n",
        "    (\"human\", \"Tell me a joke about {topic}\")\n",
        "])\n",
        "\n",
        "chain = template | llm\n",
        "response = chain.invoke({\"topic\": \"programming\"})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80eab92d",
      "metadata": {},
      "source": [
        "## Building a RAG Model from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f127c7cb",
      "metadata": {},
      "source": [
        "### Loading Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4351afa0",
      "metadata": {},
      "source": [
        "##### Uploaded a few Research paper like \"Attention is all you need!\" and asked questions regarding it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc60ec77",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter # Splits the data in recursive manner, it starts by splitting by paragraph then it splits lines the words.\n",
        "from typing import List\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def load_documents(folder_path: str) -> List[Document]:\n",
        "    documents = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        if filename.endswith('.pdf'):\n",
        "            loader = PyPDFLoader(file_path)\n",
        "        elif filename.endswith('.docx'):\n",
        "            loader = Docx2txtLoader(file_path)\n",
        "        else:\n",
        "            print(f\"Unsupported file type: {filename}\")\n",
        "            continue\n",
        "        documents.extend(loader.load())\n",
        "    return documents\n",
        "\n",
        "folder_path = \"docs\"\n",
        "documents = load_documents(folder_path)\n",
        "print(f\"Loaded {len(documents)} documents from the folder.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dac5a29f",
      "metadata": {},
      "source": [
        "### Splitting the Documents into Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4d0c093",
      "metadata": {},
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000, # Keeping the chunk size to be 1000 characters i.e. dividing our document into several chunks of 1000 characters.\n",
        "    chunk_overlap = 200, # As recursive technique can split into chunks at anywhere(maybe between a sentance or an importent/relevant point) to avoid this we set the overlapping between them so some important thing that we have should be present in both chunks.\n",
        "    length_function = len \n",
        ")\n",
        "splits = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Split the documents into {len(splits)} chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36a0ab46",
      "metadata": {},
      "outputs": [],
      "source": [
        "splits[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa30df41",
      "metadata": {},
      "source": [
        "### Embedding the Documnets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae8d4ae9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initially Using the Gemini Embedding\n",
        "# from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "# embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "# document_embedding = embeddings = embeddings.embed_documents([split.page_content for split in splits])\n",
        "\n",
        "# print(f\"Created embeddings for {len(document_embedding)} document chunks.\")\n",
        "# This was not working for some reason."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfd52334",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using the Traditional langchain community embeddings:\n",
        "from langchain_community.embeddings.sentence_transformer import HuggingFaceEmbeddings  # Can also use HuggingFaceEmbeddings\n",
        "embedding_function = HuggingFaceEmbeddings(\n",
        "    model_name=\"all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': 'cpu'}\n",
        ")\n",
        "# document_embeddings = embedding_function.embed_documents([split.page_content for split in splits])\n",
        "# document_embeddings[0][:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3056e888",
      "metadata": {},
      "source": [
        "### Creating a Vector DB store -> Chroma DB, To Store our embeddings init."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d9cf38f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "collection_name = \"my_collection\"\n",
        "vectorstore = Chroma.from_documents(collection_name= collection_name,\n",
        "                                    documents= splits, \n",
        "                                    embedding= embedding_function, \n",
        "                                    persist_directory= \"./chroma_db\")\n",
        "\n",
        "print(\"Vector store created and persisted to ./chroma_db\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61b3abfc",
      "metadata": {},
      "source": [
        "### Perform Similarity Check \n",
        "##### Look for the chunks that are similar to our query in the data (vectorstore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3894e82d",
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"What is Attention function?\"\n",
        "\n",
        "# Using MMR to force the model to find two DIFFERENT documents\n",
        "# fetch_k=10 (finds 10 closest candidates) and k=2 (returns 2 diverse ones)\n",
        "results_mmr = vectorstore.max_marginal_relevance_search(query, k=2, fetch_k=10)\n",
        "\n",
        "print(f\"\\nTop 2 most RELEVANT (and diverse) chunks for: '{query}'\\n\")\n",
        "\n",
        "for i, result in enumerate(results_mmr, 1):\n",
        "    # Note: MMR search does NOT return scores by default\n",
        "    print(f\"Result {i}:\") \n",
        "    print(f\"Source: {result.metadata.get('source', 'Unknown')}\")\n",
        "    print(f\"Content: {result.page_content[:200]}...\")\n",
        "    print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ea0cf35",
      "metadata": {},
      "source": [
        "#### Corverting the vectorstore or the data into retriever object. Using our vector store as the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a9f668e",
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "retriever_results = retriever.invoke(\"What is Attention Function?\")\n",
        "print(retriever_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbeec1b8",
      "metadata": {},
      "source": [
        "### Adding the Retriever to our Chain, Making it a complete Chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6697902e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "Question: {question}\n",
        "Answer: \"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "def docs2str(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = ( \n",
        "    # Context is whatever we are getting from the Retriever\n",
        "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser() \n",
        ")\n",
        "# Whaterver we are seeking while invoking the chain it becomes the question through RunnablePassthrough.\n",
        "# This way the prompt will get access to both our question and context. \n",
        "rag_chain.invoke(\"What is Attention Function?\")\n",
        "# Now we have the promt that could br given to the LLM to get our final answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2606da1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "question = \"What is Attention Function?\"\n",
        "response = rag_chain.invoke(question)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f5d7c18",
      "metadata": {},
      "source": [
        "### Generating the Answer by Adding LLM to the Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6af565f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAG Chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "question = \"What is Attention Function?\"\n",
        "response = rag_chain.invoke(question)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dd03279",
      "metadata": {},
      "source": [
        "### Adding Ablity to Answer Follow Up Question"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "470b570e",
      "metadata": {},
      "source": [
        "### Conversation Rag or ChatBot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c2393fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.runnables import RunnableSequence, RunnableLambda\n",
        "\n",
        "\n",
        "chat_history = []\n",
        "chat_history.extend([\n",
        "    HumanMessage(content= question),\n",
        "    AIMessage(content= response)\n",
        "])\n",
        "chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aacf53ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "contextualize_q_system_prompt = \"\"\"\n",
        "Given a chat history and the latest user question\n",
        "which might reference context in the chat history,\n",
        "formulate a standalone question which can be understood\n",
        "without the chat history. Do NOT answer the question,\n",
        "just reformulate it if needed and otherwise return it as is.\n",
        "\"\"\"\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "contextualize_chain = contextualize_q_prompt | llm | StrOutputParser()\n",
        "print(contextualize_chain.invoke({\"input\": \"How is it applied?\", \"chat_history\": chat_history}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1193189",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def history_aware_retriever(inputs):\n",
        "    standalone_question = contextualize_chain.invoke({\n",
        "        \"input\": inputs[\"input\"],\n",
        "        \"chat_history\": inputs[\"chat_history\"]\n",
        "    })\n",
        "    docs = retriever.invoke(standalone_question)\n",
        "    return {\n",
        "        \"context\": \"\\n\\n\".join([d.page_content for d in docs]),\n",
        "        \"input\": inputs[\"input\"],\n",
        "        \"chat_history\": inputs[\"chat_history\"]\n",
        "    }\n",
        "history_aware_retriever_runnable = RunnableLambda(history_aware_retriever)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f56ac76e",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant. Use the provided context to answer accurately.\"),\n",
        "    (\"system\", \"Context:\\n{context}\"),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "qa_chain = qa_prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac8adeb9",
      "metadata": {},
      "outputs": [],
      "source": [
        "rag_chain = RunnableSequence(\n",
        "    first=history_aware_retriever_runnable,\n",
        "    last=qa_chain\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b4bdc63",
      "metadata": {},
      "outputs": [],
      "source": [
        "result = rag_chain.invoke({\n",
        "    \"input\": \"How is it applied?\",\n",
        "    \"chat_history\": chat_history\n",
        "})\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d794fd17",
      "metadata": {},
      "source": [
        "### Building Multi User Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65c49d22",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "from datetime import datetime\n",
        "import uuid\n",
        "\n",
        "DB_NAME = \"rag_app.db\"\n",
        "\n",
        "def get_db_connection():\n",
        "    conn = sqlite3.connect(DB_NAME)\n",
        "    conn.row_factory = sqlite3.Row\n",
        "    return conn\n",
        "\n",
        "def create_application_logs():\n",
        "    conn = get_db_connection()\n",
        "    conn.execute('''CREATE TABLE IF NOT EXISTS application_logs\n",
        "    (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    session_id TEXT,\n",
        "    user_query TEXT,\n",
        "    gpt_response TEXT,\n",
        "    model TEXT,\n",
        "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')\n",
        "    conn.close()\n",
        "\n",
        "def insert_application_logs(session_id, user_query, gpt_response, model):\n",
        "    conn = get_db_connection()\n",
        "    conn.execute('INSERT INTO application_logs (session_id, user_query, gpt_response, model) VALUES (?, ?, ?, ?)',\n",
        "                 (session_id, user_query, gpt_response, model))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def get_chat_history(session_id):\n",
        "    conn = get_db_connection()\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute('SELECT user_query, gpt_response FROM application_logs WHERE session_id = ? ORDER BY created_at', (session_id,))\n",
        "    messages = []\n",
        "    for row in cursor.fetchall():\n",
        "        messages.extend([\n",
        "            {\"role\": \"human\", \"content\": row['user_query']},\n",
        "            {\"role\": \"ai\", \"content\": row['gpt_response']}\n",
        "        ])\n",
        "    conn.close()\n",
        "    return messages\n",
        "\n",
        "# Initialize the database\n",
        "create_application_logs()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4a2941a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage for a new user\n",
        "session_id = str(uuid.uuid4())\n",
        "question1 = \"What is Attention Function?\"\n",
        "chat_history = get_chat_history(session_id)\n",
        "answer1 = rag_chain.invoke({\"input\": question1, \"chat_history\": chat_history}) \n",
        "insert_application_logs(session_id, question1, answer1, \"gpt-3.5-turbo\")\n",
        "print(f\"Human: {question1}\")\n",
        "print(f\"AI: {answer1}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1310082",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of a follow-up question\n",
        "question2 = \"How is it applied?\"\n",
        "chat_history = get_chat_history(session_id)\n",
        "answer2 = rag_chain.invoke({\"input\": question2, \"chat_history\": chat_history})\n",
        "insert_application_logs(session_id, question2, answer2, \"gpt-3.5-turbo\")\n",
        "print(f\"Human: {question2}\")\n",
        "print(f\"AI: {answer2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ca12692",
      "metadata": {},
      "source": [
        "### Voila! we have our RAG CHAIN ready and woking!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}